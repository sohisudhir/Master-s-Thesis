# -*- coding: utf-8 -*-
"""Combined_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z2qYEQ15gT32q9WFUJ2-LBhaDSglVI66
"""

# Setup & Config
import transformers
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup

import torch
from torch.nn import CrossEntropyLoss, MSELoss
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

import numpy as np
import pandas as pd
import random
import copy
import csv
import re
import argparse
import os

from sklearn.model_selection import train_test_split
from sklearn import metrics

from scipy.stats import pearsonr
from scipy.stats import kendalltau
from scipy.stats import spearmanr

# RANDOM_SEED = 42
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

"""## **Data Preparation**"""

class AbuseDataset(Dataset):

  def __init__(self, reviews, targets, c1,c2,c3, c_num, tokenizer, max_len, ids):
    self.reviews = reviews
    self.targets = targets
    self.c1 = c1
    self.c2 = c2
    self.c3 = c3
    self.c_num = c_num
    self.tokenizer = tokenizer
    self.max_len = max_len
    self.ids = ids
  
  def __len__(self):
    return len(self.reviews)
  
  def __getitem__(self, item):
    c=["[PAD]","[PAD]","[PAD]"]
    review = str(self.reviews[item])
    target = self.targets[item]
    c_num = self.c_num[item]
    c[0] = str(self.c1[item])
    c[1] = str(self.c2[item])
    c[2] = str(self.c3[item])
    encoding = self.tokenizer.encode_plus(
      review,
      add_special_tokens=True,
      truncation=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )
    idx = self.ids[item]
    context_input_ids = []
    context_attention_mask = []
    for i in range(0,3):
      encoding_context = self.tokenizer.encode_plus(
      c[i],
      add_special_tokens=True,
      truncation=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt')
      context_input_ids.append(encoding_context['input_ids'].flatten())
      context_attention_mask.append(encoding_context['attention_mask'].flatten())

    return {
      'review_text': review,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.float),
      'context_input_ids': torch.stack(context_input_ids),
      'context_attention_masks': torch.stack(context_attention_mask),
      'context_num': c_num,
      'ids': idx
    }

class EmotionDataset(Dataset):

  def __init__(self, tweets, targets, tokenizer, max_len):
    self.tweets = tweets
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len
  
  def __len__(self):
    return len(self.tweets)

  def __getitem__(self, item):
    tweet = str(self.tweets[item])
    target = self.targets[item]

    encoding = self.tokenizer.encode_plus(
      tweet,
      add_special_tokens=True,
      truncation = True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )

    return {
      'tweet_text': tweet,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long)
    }

class GeneralAttention(nn.Module):

  def __init__(self, sparsemax=False):
      super().__init__()
      self.linear = nn.Linear(768, 1)
      # self.normaliser = masked_softmax
      self.weights = []
  def masked_softmax(self, vector, mask):
    while mask.dim() < vector.dim():
      mask = mask.unsqueeze(1)
    # To limit numerical errors from large vector elements outside the mask, we zero these out.
    result = torch.nn.functional.softmax(vector * mask, dim=-1)
    result = result * mask
    result = result / (
        result.sum(dim=-1, keepdim=True) + 1e-4
    )
    return result

  def forward(self, context, masks, batch_size):
      context = torch.cat(context, dim=1)
      context = context.reshape(-1,3,768)
      weights = self.linear(context).squeeze(-1)
      weights = self.masked_softmax(weights, masks)
      context = torch.bmm(weights.unsqueeze(dim=1), context)

      return context, weights

def create_maintask_data_loader(df_train, tokenizer, max_len, batch_size, flag = 0):

  ds = AbuseDataset(reviews= df_train.comment.to_numpy(), targets= df_train.Score.to_numpy(), 
                    c1 = df_train.context1.to_numpy(), c2 = df_train.context2.to_numpy(),
                    c3 = df_train.context3.to_numpy(), c_num = df_train.context_num.to_numpy(), 
                    tokenizer = tokenizer, max_len = max_len, ids = df_train.idx.to_numpy())

  if(flag == 0):
    return DataLoader(ds,
      batch_size=batch_size,
      num_workers=4
    )
  else:
    return DataLoader(ds,
      batch_size=batch_size,
      num_workers=4,
      shuffle = True
    )

def create_auxtask_data_loader(df, tokenizer, max_len, batch_size, flag = 0):

  anger = df.anger.to_numpy()
  anticipation = df.anticipation.to_numpy()
  disgust = df.disgust.to_numpy()
  fear = df.fear.to_numpy()
  joy = df.joy.to_numpy()
  love = df.love.to_numpy()
  optimism = df.optimism.to_numpy()
  pessimism = df.pessimism.to_numpy()
  sadness = df.sadness.to_numpy()
  surprise = df.surprise.to_numpy()
  trust = df.trust.to_numpy()
  emotion = np.stack((anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust), axis = 1)
  ds = EmotionDataset(
    tweets=df.Tweet.to_numpy(),
    targets= emotion,
    tokenizer=tokenizer,
    max_len=max_len
  )

  if(flag == 0):
    return DataLoader(ds,
      batch_size=batch_size,
      num_workers=4
    )
  else:
    return DataLoader(ds,
      batch_size=batch_size,
      num_workers=4,
      shuffle = True
    )

def clean_tweets(csvf): 
  fname = 'cleaned_' + csvf
  with open(csvf, 'r', encoding = 'utf-8') as c, open(fname, 'w', encoding = 'UTF-8') as w:
    reader = csv.reader(c, delimiter = '\t')
    writer = csv.writer(w, delimiter = '\t')
    for i,row in enumerate(reader):
      if(i == 0):
        writer.writerow(row)
        continue
      row[1] = row[1].lower()
      row[1] = re.sub(r"#(\w+)", "HASHTAG",  row[1])
      row[1] = re.sub(r"(^|[^@\w])@(\w{1,15})", "_MTN_",  row[1])
      row[1] = re.sub(r"https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+", "_URL_",  row[1])
      writer.writerow(row)
    
    c.close()
    w.close()

def prepare_data(abuse_files, sent_files, config):
  
  #Requirements
  BATCH_SIZE = config['batch_size']
  MAX_LEN = config['max_len']
  PRE_TRAINED_MODEL_NAME = config['PRE_TRAINED_MODEL_NAME']
  tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
  
  # Arranging data loaders for main task
  a = abuse_files[0]
  # b = abuse_files[1]
  c = abuse_files[2]
  df_train = pd.read_csv(a)
  # df_val = pd.read_csv(b)
  df_test = pd.read_csv(c)
  # df_train = df_train[0:1000]
  # df_val = df_train[0:200]
  # df_test = df_train[0:800]

  print('Dimensions of abuse file')
  print(df_train.shape, 0, df_test.shape)

  data_loader_main = create_maintask_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, 1)
  # val_data_loader_main = create_maintask_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
  test_data_loader_main = create_maintask_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

  # Arranging data loaders for auxiliary task -- SEMEVAL2018A
  a = sent_files[0]
  b = sent_files[1]
  c = sent_files[2]
  clean_tweets(a)
  clean_tweets(b)
  clean_tweets(c)
  df_train = pd.read_csv('cleaned_' + a, sep = '\t')
  # df_train = df_train[0:100]
  df_val = pd.read_csv('cleaned_' + b, sep = '\t')
  # df_val = df_val[0:20]
  df_test = pd.read_csv('cleaned_' + c, sep = '\t')
  # df_test = df_test[0:80]
  print('Dimensions of sentiment file')
  print(df_train.shape, df_val.shape, df_test.shape)

  data_loader_aux = create_auxtask_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, 1)
  val_data_loader_aux = create_auxtask_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
  test_data_loader_aux = create_auxtask_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

  dataloaders = {'main_train': data_loader_main, 'main_val': [], 'main_test': test_data_loader_main, 'aux_train': data_loader_aux, 'aux_val': val_data_loader_aux, 'aux_test':  test_data_loader_aux }
  
  return dataloaders

"""## **MODELS**"""

class MSLELoss(nn.Module):
  def __init__(self):
    super().__init__()
    self.mse = nn.MSELoss(reduction = 'sum')
        
  def forward(self, pred, actual):
    return self.mse(torch.log(pred + 1.00005), torch.log(actual + 1.00005))

class multitask_conversation_model(nn.Module):

  def __init__(self, config): #num_labels, num_emotions, attention_dropout, fc_dropout):
    super(multitask_conversation_model, self).__init__()
    self.num_labels = config['abuse_classes']
    self.num_emotions = config['sent_classes']
    self.device = config['device']
    PRE_TRAINED_MODEL_NAME = config['PRE_TRAINED_MODEL_NAME']
    self.b_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
    self.bert = AdaptedBertModel(self.b_model, True, True, config['bert_dropout'], config['fc_dropout'])
    self.bert_config = self.bert.config
    self.attention = GeneralAttention()
    self.attention.to(self.device)
    self.main_regression = nn.Linear(self.bert_config.hidden_size, self.num_labels)
    self.aux_classifier = nn.Linear(self.bert_config.hidden_size, self.num_emotions)
    del(self.b_model)

  def forward(self, input_ids, token_type_ids=None, attention_mask=None, main_task=True, targets = None):
    if main_task:
      outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, mode='main_task')
      # pooled_output = self.bert.pooler(outputs)
      pooled_output = outputs.mean(dim = 1)
      return pooled_output
    else:
      outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, mode= 'auxiliary_task')
      # pooled_output = self.bert.pooler(outputs)
      pooled_output = outputs.mean(dim = 1)
      return pooled_output

class AdaptedBertModel(nn.Module):
  def __init__(self, model, main_task, auxiliary_task, attention_dropout, fc_dropout):
    super().__init__()
    self.embeddings = model.embeddings
    self.encoder = BertEncoder(model.encoder.layer, main_task, auxiliary_task, attention_dropout, fc_dropout)
    self.config = model.config
    self.pooler = model.pooler
  
  def forward(self, input_ids, token_type_ids=None, attention_mask=None,
              mode="main_task"):
    if attention_mask is None:
        attention_mask = torch.ones_like(input_ids)
    if token_type_ids is None:
        token_type_ids = torch.zeros_like(input_ids)

    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
    extended_attention_mask = extended_attention_mask.to(
        dtype=next(self.parameters()).dtype
    )
    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
    embeddings = self.embeddings(input_ids, token_type_ids)
    embeddings = self.encoder(embeddings, extended_attention_mask, mode)
    return embeddings

# Hard parameter sharing setup : All layers but the last are shared
# Last layer is task-specific

class BertEncoder(nn.Module):

  def __init__(self, layers, main_task, auxiliary_task, attention_dropout, fc_dropout):
    super().__init__()
    self.layers = layers[:-1]
    self.output_attentions = False
    for layer in self.layers:
        layer.attention.self.dropout = nn.Dropout(attention_dropout)
    if main_task:
        self.layer_left = copy.deepcopy(layers[-1])
    if auxiliary_task:
        self.layer_right = copy.deepcopy(layers[-1])

  def forward(self, hidden, attention_mask, mode):
    all_attentions = ()
    for layer in self.layers:
        hidden = layer(hidden, attention_mask)
        if self.output_attentions:
            all_attentions = all_attentions + (hidden[1],)
        hidden = hidden[0]
    if mode == "main_task":
        hidden = self.layer_left(hidden, attention_mask)
    elif mode == "auxiliary_task":
        hidden = self.layer_right(hidden, attention_mask)
    outputs = hidden[0]
    if self.output_attentions:
        outputs = outputs + (all_attentions,)
    return outputs

def evaluation_metrics(preds, targets):
  
  with torch.no_grad():
    tp = torch.zeros(preds.shape[1])
    tn = torch.zeros(preds.shape[1])
    fp = torch.zeros(preds.shape[1])
    fn = torch.zeros(preds.shape[1])
    for n,pred in enumerate(preds):
      for j,pr in enumerate(pred):
        t = targets[n][j]
        if(pr == 0):
          if(t == 0):
            tn[j] += 1
          else:
            fn[j] += 1
        elif(pr == 1):
          if(t == 0):
            fp[j] += 1
          else:
            tp[j] += 1
    #Micro
    num = torch.sum(tp)
    deno_prec = torch.zeros(preds.shape[1])
    deno_rec = torch.zeros(preds.shape[1])
    for j,val in enumerate(deno_prec):
      deno_prec[j] = tp[j] + fp[j]
      deno_rec[j] = tp[j] + fn[j]
    den = torch.sum(deno_prec)
    if(den == 0):
      micro_precision = 0
    else:
      micro_precision = num.item()/den.item()
    den = torch.sum(deno_rec)
    if(den == 0):
      micro_recall = 0
    else:
      micro_recall = num.item()/den.item()
    numerator = 2 * micro_precision * micro_recall
    denominator = micro_precision + micro_recall
    if(denominator == 0):
      micro_f1 = 0
    else:
      micro_f1 = numerator/denominator
    # print(micro_precision, micro_recall, micro_f1)

    #MACRO

    precision = torch.zeros(preds.shape[1])
    recall = torch.zeros(preds.shape[1])
    for j,val in enumerate(precision):
      if(tp[j] + fp[j] == 0):
        precision[j] = 0
      else:
        precision[j] = tp[j]/(tp[j] + fp[j])
      if(tp[j] + fn[j] == 0):
        recall[j] = 0
      else:
        recall[j] = tp[j]/(tp[j] + fn[j])
    f1 =  torch.zeros(preds.shape[1])
    for j,val in enumerate(f1):
      num = 2 * precision[j] * recall[j]
      deno = precision[j] + recall[j]
      if(deno == 0):
        f1[j] = 0
      else:
        f1[j] = num/deno
    macro_precision = torch.mean(precision)
    macro_recall = torch.mean(recall)
    macro_f1 = torch.mean(f1)
    # print(macro_precision, macro_recall, macro_f1)

  return micro_f1, macro_f1

def eval_model(model, data_loader, device, mode):
  
  model = model.to(device)
  model = model.eval()
  loss_fn_main = nn.MSELoss().to(device)
  loss_fn_aux = nn.BCEWithLogitsLoss().to(device)

  if(mode == 'main_task'):
    p = []
    t = []
    loss_m = []
    ids = []
    emotion_pred = []
    c1_pred = []
    c2_pred = []
    c3_pred = []
    with torch.no_grad():
      for d in data_loader:
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        targets = d["targets"].to(device)
        context_input_ids = d["context_input_ids"].to(device)
        context_attention_masks = d["context_attention_masks"].to(device)
        context_num = d['context_num'].to(device)
        outputs = model.forward(input_ids=input_ids, token_type_ids = None, attention_mask=attention_mask, main_task = True)
        out_encoding = []

        for i in range(len(context_input_ids)):
          c = model.forward(input_ids=context_input_ids[i].to(device),attention_mask=context_attention_masks[i].to(device))
          out_encoding.append(c)
          ops = model(input_ids=context_input_ids[i].to(device), token_type_ids = None, attention_mask=context_attention_masks[i].to(device), main_task = False)
          c1,c2,c3 = torch.unbind(ops, dim = 0)
          
          logits = model.module.aux_classifier(c1.unsqueeze(dim = 0))
          predictions = torch.sigmoid(logits)
          preds = torch.gt(predictions, 0.5).int()
          c1_pred.extend(preds)
          logits = model.module.aux_classifier(c2.unsqueeze(dim = 0))
          predictions = torch.sigmoid(logits)
          preds = torch.gt(predictions, 0.5).int()
          c2_pred.extend(preds)
          logits = model.module.aux_classifier(c3.unsqueeze(dim = 0))
          predictions = torch.sigmoid(logits)
          preds = torch.gt(predictions, 0.5).int()
          c3_pred.extend(preds)
          
        mask = torch.zeros([input_ids.shape[0],3])
        for i in range(len(context_num)):
          arr = np.zeros(3)
          arr[:context_num[i]] = 1
          mask[i] = torch.tensor(arr)
        mask = mask.to(device)
        weighted, weights = model.module.attention.forward(out_encoding, mask, config['batch_size'])
        main_context = outputs.add(weighted.squeeze(dim=1))
        val = model.module.main_regression(main_context)
        predictions = torch.tanh(val)
        loss = loss_fn_main(predictions.squeeze(dim = 1), targets)
        p.extend(predictions.squeeze(dim=1).to('cpu').detach().numpy())
        t.extend(targets.to('cpu').detach().numpy())
        ids.extend(d["ids"].to('cpu').detach().numpy())
        loss_m.append(loss.item())
        ops = model(input_ids=input_ids, token_type_ids = None, attention_mask=attention_mask, main_task = False, targets = targets)
        logits = model.module.aux_classifier(ops)
        predictions = torch.sigmoid(logits)
        preds = torch.gt(predictions, 0.5).int()
        emotion_pred.extend(preds)
    
    with open('testing_preds_mtl_combo.csv', 'a', encoding = 'utf-8') as f:
        writer = csv.writer(f)
        # writer.writerow(['ID', 'Prediction', 'Target'])
        row = []
        for i,idx in enumerate(ids):
          row.append(idx)
          row.append(p[i])
          row.append(t[i])
          row.append(emotion_pred[i].to('cpu').detach().numpy())
          row.append(c1_pred[i].to('cpu').detach().numpy())
          row.append(c2_pred[i].to('cpu').detach().numpy())
          row.append(c3_pred[i].to('cpu').detach().numpy())

          writer.writerow(row)
          row = []
    f.close()


    pear = pearsonr(np.array(t),np.array(p))
    spear = spearmanr(np.array(t),np.array(p))
    tau = kendalltau(np.array(t),np.array(p))
    loss = np.mean(loss_m)
    return pear[0], spear[0], tau[0], loss
  
  elif(mode == 'auxiliary_task'):
    accuracies = []
    loss_a = []
    micro_f1 = []
    macro_f1 = []
    with torch.no_grad():
      for d in data_loader:
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        targets = d["targets"].to(device)
        # logits = model(input_ids=input_ids, token_type_ids = None, attention_mask=attention_mask, main_task = False, targets = targets)
        ops = model(input_ids=input_ids, token_type_ids = None, attention_mask=attention_mask, main_task = False, targets = targets)
        logits = model.module.aux_classifier(ops)
        predictions = torch.sigmoid(logits)
        loss = loss_fn_aux(logits.float(), targets.float())
        loss_a.append(loss.item())
        preds = torch.gt(predictions, 0.5).int()
        mic_f1, mac_f1 = evaluation_metrics(preds, targets)
        micro_f1.append(mic_f1)
        macro_f1.append(mac_f1)
    avg_micro_f1 = np.mean(micro_f1)
    avg_macro_f1 = np.mean(macro_f1)
    loss = np.mean(loss_a)
    return avg_micro_f1, avg_macro_f1, loss

def train_epoch(model, dataloaders, device, config):
  
  model = model.to(config['device'])
  model = model.train()
  least_loss = 100.0

  
  data_loader_main = dataloaders['main_train']
  data_loader_aux = dataloaders['aux_train']
  val_data_loader_main = dataloaders['main_val']
  val_data_loader_aux = dataloaders['aux_val']

  loss_fn_main = nn.MSELoss().to(config['device'])
  loss_fn_aux =  nn.BCEWithLogitsLoss().to(config['device'])
  
  device = config['device']
  optimizer_main = AdamW(model.module.bert.parameters(), lr = config['lr_main'], weight_decay= 1e-4, correct_bias=False)
  optimizer_main2 = AdamW(model.module.main_regression.parameters(), lr = config['lr_main']*10, weight_decay= 1e-4, correct_bias=False)
  optimizer_main3 = AdamW(model.module.attention.parameters(), lr = config['lr_main']*10, weight_decay= 1e-4, correct_bias=False)
  
  # optimizer_main = torch.optim.Adam(model.parameters(), lr = 0.001)
  # optimizer_aux = torch.optim.Adam(model.parameters(), lr = config['lr_aux'])
  optimizer_aux = AdamW(model.module.bert.parameters(), lr = config['lr_aux'], weight_decay= 1e-4, correct_bias=False)
  optimizer_aux2 = AdamW(model.module.aux_classifier.parameters(), lr = config['lr_aux']*10, weight_decay= 1e-4, correct_bias=False)
  
  total_steps = len(data_loader_main) * config['num_epochs']
  scheduler_main = get_linear_schedule_with_warmup(
  optimizer_main,
  num_warmup_steps=0,
  num_training_steps=total_steps
)
  total_steps = len(data_loader_aux) * config['num_epochs']
  scheduler_aux = get_linear_schedule_with_warmup(
  optimizer_aux,
  num_warmup_steps=0,
  num_training_steps=total_steps
)
  # optimizer_main = AdamW(model.parameters(), lr = config['lr_main'], weight_decay= 1e-4, correct_bias=False)
  # optimizer_aux = AdamW(model.parameters(), lr = config['lr_aux'], weight_decay= 1e-4, correct_bias=False)
  # total_steps = len(data_loader_main) * config['num_epochs']
  # scheduler = get_linear_schedule_with_warmup(
  # optimizer_main,
  # num_warmup_steps=0,
  # num_training_steps=total_steps
# )

  coin_flips = []
  #main_task
  for i in range(len(data_loader_main)):
    coin_flips.append(0)
  #auxiliary task
  for i in range(len(data_loader_aux)):
    coin_flips.append(1)

  val_counter = 0
  for epoch in range(config['num_epochs']):
    if(epoch >= 3):
      print('Freezing Bert!')
      for param in model.module.bert.encoder.parameters():
        param.requires_grad = False

    print("Starting epoch {}".format(epoch))
    random.shuffle(coin_flips)
    loss_m = []
    loss_a = []
    p = []
    t = []
    micro_f1 = [] 
    macro_f1 = []
    accuracies = []
    main_dl = iter(data_loader_main)
    aux_dl = iter(data_loader_aux)
    for i in coin_flips:
      if(i == 0):
        #MAIN_TASK
        try:
          d = next(main_dl)
        except:
          main_dl = iter(data_loader_main)
          d = next(main_dl)
        # print('In main task')
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        targets = d["targets"].to(device)
        context_input_ids = d["context_input_ids"].to(device)
        context_attention_masks = d["context_attention_masks"].to(device)
        context_num = d['context_num'].to(device)
        outputs = model.forward(input_ids=input_ids, token_type_ids = None, attention_mask=attention_mask, main_task = True)
        out_encoding = []
        for i in range(len(context_input_ids)):
          c = model.forward(input_ids=context_input_ids[i].to(device),attention_mask=context_attention_masks[i].to(device))
          out_encoding.append(c)
        mask = torch.zeros([input_ids.shape[0],3]).to(device)
        for i in range(len(context_num)):
          arr = np.zeros(3)
          arr[:context_num[i]] = 1
          mask[i] = torch.tensor(arr)
        weighted,_ = model.module.attention.forward(out_encoding, mask, config['batch_size'])
        main_context = outputs.add(weighted.squeeze(dim=1))
        val = model.module.main_regression(main_context)
        predictions = torch.tanh(val)
        loss = loss_fn_main(predictions.squeeze(dim = 1), targets)
        p.extend(predictions.squeeze(dim=1).to('cpu').detach().numpy())
        t.extend(targets.to('cpu').detach().numpy())
        loss_m.append(loss.item())
        loss.backward()
        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer_main.step()
        optimizer_main2.step()
        optimizer_main3.step()

        scheduler_main.step()
        optimizer_main.zero_grad()
        optimizer_main2.zero_grad()
        optimizer_main3.zero_grad()

        val_counter += 1
      else:
        try:
          d = next(aux_dl)
        except:
          aux_dl = iter(data_loader_aux)
          d = next(aux_dl)
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        targets = d["targets"].to(device)
        ops = model(input_ids=input_ids, token_type_ids = None, attention_mask=attention_mask, main_task = False, targets = targets)
        logits = model.module.aux_classifier(ops)
        predictions = torch.sigmoid(logits)
        loss = loss_fn_aux(logits.float(), targets.float())
        loss_a.append(loss.item())
        loss = loss * 0.4
        loss.backward()
        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer_aux.step()
        optimizer_aux2.step()

        scheduler_aux.step()
        optimizer_aux.zero_grad()
        optimizer_aux2.zero_grad()
        preds = torch.gt(predictions, 0.5).int()
        mic_f1, mac_f1 = evaluation_metrics(preds, targets)
        micro_f1.append(mic_f1)
        macro_f1.append(mac_f1)
        # print('aux task completed')
        val_counter +=1


    pear = pearsonr(np.array(t),np.array(p))
    spear = spearmanr(np.array(t),np.array(p))
    tau = kendalltau(np.array(t),np.array(p))
    avg_micro_f1 = np.mean(micro_f1)
    avg_macro_f1 = np.mean(macro_f1)
    print("Epoch {}. Training Pearson {}.Training Pearson {}.Training Spearman {} Training Loss {}".format(epoch, pear[0], spear[0], tau[0], np.mean(loss_m)))
    print("Epoch {}. Training Micro F1 {}.Training Macro F1 {}.Training Loss {}".format(epoch, avg_micro_f1, avg_macro_f1, np.mean(loss_a)))
    
    # pearson, spearman, kendall, loss = eval_model(model, val_data_loader_main, device, mode = 'main_task')
    # print("MAIN: Epoch {}. Validation Pearson {}.Validation Spearman {}. Validation Kendall {}. Validation Loss {}".format(epoch, pearson, spearman, kendall,loss))
    # if(loss < least_loss):
    #   print('Saving best model')
    #   least_loss = loss
    #   state = {'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer_main': optimizer_main.state_dict(),
    #   'optimizer_aux': optimizer_aux.state_dict()}#, 'scheduler_main': scheduler_main, 'scheduler_aux': scheduler_aux}
    #   torch.save(state, 'mtl_best_model.ckpt')
    
    avg_micro_f1, avg_macro_f1, loss = eval_model(model, val_data_loader_aux, device, mode = 'auxiliary_task')
    print("AUX: Epoch {}.Validation Micro F1 {}.Validation Macro F1 {}. Validation Loss {}".format(epoch, avg_micro_f1, avg_macro_f1, loss))
    
    state = {'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer_main': optimizer_main.state_dict(),
      'optimizer_aux': optimizer_aux.state_dict()}#, 'scheduler_main': scheduler_main, 'scheduler_aux': scheduler_aux}
    print('Saving last model')
    torch.save(state, 'mtl_combo_last_model.ckpt')

"""# **Calling the model**"""

if __name__ == "__main__":

  parser = argparse.ArgumentParser(description="Enter args")
  parser.add_argument('--PRE_TRAINED_MODEL_NAME', default="bert-base-cased", type=str)
  parser.add_argument('--batch_size', default=16, type=int)
  parser.add_argument('--max_len', default=200, type=int)
  parser.add_argument('--abuse_classes', default=1, type=int)
  parser.add_argument('--sent_classes', default=11, type=int)
  parser.add_argument('--bert_dropout', default=0.1, type=float)
  parser.add_argument('--fc_dropout', default=0.4, type=float)
  parser.add_argument('--num_epochs', default=5, type=int)
  parser.add_argument('--lr_main', default=3e-5, type=float)
  parser.add_argument('--lr_aux', default=3e-5, type=float)
  parser.add_argument('--wd', default=1e-4, type=float)
  parser.add_argument('--csv_index', default = 1, type = int)

  args = parser.parse_args()

  print('************************************************************************************')
  # print('bert_dropout', bert_dropout, 'fc_dropout', fc_dropout)
  config = {
      'PRE_TRAINED_MODEL_NAME': 'bert-base-cased',
      'batch_size': args.batch_size,
      'max_len': args.max_len,
      'abuse_classes': args.abuse_classes,
      'sent_classes': args.sent_classes,
      'bert_dropout': args.bert_dropout,
      'fc_dropout': args.fc_dropout,
      'device': torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),
      'num_epochs': args.num_epochs,
      'lr_main':args.lr_main,
      'lr_aux': args.lr_aux

  }
  train_file = 'train' + str(args.csv_index) + '.csv'
  test_file = 'test' + str(args.csv_index) + '.csv'

  abuse_files = [train_file, '', test_file]
  # abuse_files = ['train.csv', 'val.csv', 'test.csv']#'comm_uqs_with_convo.csv' #'main_cmv_datatset_10000.csv'
  sent_files = ['train.tsv', 'dev.tsv', 'test.tsv']
  dataloaders = prepare_data(abuse_files, sent_files, config)
  model = multitask_conversation_model(config)
  device = config['device']
  print('DEVICE IS', device)
  if torch.cuda.device_count() > 1:
    print("Let's use", torch.cuda.device_count(), "GPUs!")
    model = nn.DataParallel(model).to(device)
  train_epoch(model, dataloaders, device, config)
  print('End of training....')
  test_data_loader_main = dataloaders['main_test']
  test_data_loader_aux = dataloaders['aux_test']
  # checkpoint = torch.load('mtl_best_model.ckpt')
  # test_model = multitask_conversation_model(config)
  # if torch.cuda.device_count() > 1:
  #   print("Let's use", torch.cuda.device_count(), "GPUs!")
  #   test_model = nn.DataParallel(test_model).to(device)
  # test_model = test_model.to(device)
  # test_model.load_state_dict(checkpoint['state_dict'])
  # print('Loaded best model')
  # pearson, spearman, kendall, loss = eval_model(test_model, test_data_loader_main, device, mode = 'main_task')
  # print("MAIN:. Test Pearson {}.Test Spearman {}.Test kendall {}. Test Loss {}".format(pearson, spearman, kendall, loss))
  # avg_micro_f1, avg_macro_f1, loss = eval_model(test_model, test_data_loader_aux, device, mode = 'auxiliary_task')
  # print("AUX: Test Micro F1 {}.Test Macro F1 {}. Test Loss {}".format(avg_micro_f1, avg_macro_f1, loss))
  print('Loaded last model(Sanity check)')
  pearson, spearman, kendall, loss = eval_model(model, test_data_loader_main, device, mode = 'main_task')
  print("MAIN:. Test Pearson {}.Test Spearman {}.Test kendall {}. Test Loss {}".format(pearson, spearman, kendall, loss))
  avg_micro_f1, avg_macro_f1, loss = eval_model(model, test_data_loader_aux, device, mode = 'auxiliary_task')
  print("AUX: Test Micro F1 {}.Test Macro F1 {}. Test Loss {}".format(avg_micro_f1, avg_macro_f1, loss))

  os.remove(train_file)
  os.remove(test_file)
  os.remove('mtl_combo_last_model.ckpt')